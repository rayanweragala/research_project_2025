{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "fd262ad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import json\n",
    "import os\n",
    "import librosa\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "44e7bc59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "AUDIO_DIR = \"audio_augmented\"  # Directory containing WAV files\n",
    "SAMPLE_RATE = 16000\n",
    "MAX_AUDIO_LENGTH = 5000    # Max audio duration in ms (5 seconds)\n",
    "N_MFCC = 13\n",
    "MAX_TEXT_LENGTH = 50 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "d6936066",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(json_path):\n",
    "    \"\"\"Load JSON data file\"\"\"\n",
    "    try:\n",
    "        with open(json_path) as f:\n",
    "            data = json.load(f)\n",
    "        print(f\"Loaded {len(data)} entries from {json_path}\")\n",
    "        return data\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading data: {e}\")\n",
    "        return {}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "3a128c19",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_mfcc(filename, max_pad_len=100):\n",
    "    \"\"\"Extract MFCC features from audio file\"\"\"\n",
    "    filepath = os.path.join(AUDIO_DIR, filename)\n",
    "    \n",
    "    try:\n",
    "        y, sr = librosa.load(filepath, sr=SAMPLE_RATE)\n",
    "        \n",
    "        # Ensure audio is 5 seconds (pad/trim)\n",
    "        max_len = int(SAMPLE_RATE * (MAX_AUDIO_LENGTH / 1000))\n",
    "        if len(y) > max_len:\n",
    "            y = y[:max_len]\n",
    "        else:\n",
    "            y = np.pad(y, (0, max(0, max_len - len(y))), 'constant')\n",
    "        \n",
    "        # Extract MFCC features\n",
    "        mfccs = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=N_MFCC)\n",
    "        mfccs = np.transpose(mfccs)  # Shape: (time, n_mfcc)\n",
    "        \n",
    "        # Pad/trim to fixed timesteps\n",
    "        if mfccs.shape[0] > max_pad_len:\n",
    "            mfccs = mfccs[:max_pad_len, :]\n",
    "        else:\n",
    "            pad_width = [(0, max_pad_len - mfccs.shape[0]), (0, 0)]\n",
    "            mfccs = np.pad(mfccs, pad_width, mode='constant')\n",
    "        \n",
    "        return mfccs\n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting MFCC from {filename}: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "8630860d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def create_vocab(data):\n",
    "    \"\"\"Create character vocabulary from text data\"\"\"\n",
    "    chars = set()\n",
    "    for item in data.values():\n",
    "        chars.update(item['text'])\n",
    "    \n",
    "    # Sort characters for consistency\n",
    "    sorted_chars = sorted(chars)\n",
    "    char_to_num = {char: idx + 1 for idx, char in enumerate(sorted_chars)}  # 0 reserved for padding\n",
    "    char_to_num['<BLANK>'] = 0  # CTC blank token\n",
    "    num_to_char = {idx: char for char, idx in char_to_num.items()}\n",
    "    \n",
    "    print(f\"Vocabulary size: {len(char_to_num)} characters\")\n",
    "    print(f\"Characters: {sorted_chars[:20]}...\")  # Show first 20 chars\n",
    "    \n",
    "    return char_to_num, num_to_char\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "7307b947",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_dataset(data, char_to_num, max_samples=None):\n",
    "    \"\"\"Prepare dataset for training\"\"\"\n",
    "    X = []\n",
    "    y = []\n",
    "    skipped_files = []\n",
    "    processed_count = 0\n",
    "\n",
    "    items = list(data.items())\n",
    "    if max_samples:\n",
    "        items = items[:max_samples]\n",
    "\n",
    "    print(f\"Processing {len(items)} audio files...\")\n",
    "    \n",
    "    for i, (key, item) in enumerate(items):\n",
    "        if i % 100 == 0:\n",
    "            print(f\"Processed {i}/{len(items)} files...\")\n",
    "            \n",
    "        filepath = os.path.join(AUDIO_DIR, item['newfn'])\n",
    "        \n",
    "        # Skip if audio file doesn't exist\n",
    "        if not os.path.exists(filepath):\n",
    "            skipped_files.append((key, f\"File not found: {item['newfn']}\"))\n",
    "            continue\n",
    "            \n",
    "        # Skip if text is too long\n",
    "        if len(item['text']) > MAX_TEXT_LENGTH:\n",
    "            skipped_files.append((key, f\"Text too long: {len(item['text'])} > {MAX_TEXT_LENGTH}\"))\n",
    "            continue\n",
    "            \n",
    "        try:\n",
    "            # Extract audio features\n",
    "            mfcc = extract_mfcc(item['newfn'])\n",
    "            if mfcc is None:\n",
    "                skipped_files.append((key, \"Failed to extract MFCC\"))\n",
    "                continue\n",
    "                \n",
    "            # Encode text\n",
    "            text_encoded = []\n",
    "            for char in item['text']:\n",
    "                if char in char_to_num:\n",
    "                    text_encoded.append(char_to_num[char])\n",
    "                else:\n",
    "                    # Skip unknown characters or replace with blank\n",
    "                    continue\n",
    "                    \n",
    "            if len(text_encoded) == 0:\n",
    "                skipped_files.append((key, \"No valid characters in text\"))\n",
    "                continue\n",
    "                \n",
    "            X.append(mfcc)\n",
    "            y.append(text_encoded)\n",
    "            processed_count += 1\n",
    "            \n",
    "        except Exception as e:\n",
    "            skipped_files.append((key, f\"Processing error: {str(e)}\"))\n",
    "            continue\n",
    "\n",
    "    if len(X) == 0:\n",
    "        print(\"ERROR: No valid samples found!\")\n",
    "        print(\"Skipped files:\")\n",
    "        for key, reason in skipped_files[:10]:  # Show first 10 errors\n",
    "            print(f\"  {key}: {reason}\")\n",
    "        return np.array([]), np.array([])\n",
    "\n",
    "    # Convert to numpy arrays\n",
    "    X = np.array(X)\n",
    "    \n",
    "    # Pad text sequences\n",
    "    y_padded = pad_sequences(y, padding='post', value=0, maxlen=MAX_TEXT_LENGTH)\n",
    "    \n",
    "    print(f\"Successfully processed {processed_count} files\")\n",
    "    print(f\"Skipped {len(skipped_files)} files\")\n",
    "    print(f\"Final shapes: X={X.shape}, y={y_padded.shape}\")\n",
    "    \n",
    "    if len(skipped_files) > 0:\n",
    "        print(\"\\nFirst few skipped files:\")\n",
    "        for key, reason in skipped_files[:5]:\n",
    "            print(f\"  {key}: {reason}\")\n",
    "    \n",
    "    return X, y_padded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "777031ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(input_dim, output_dim, rnn_units=128):\n",
    "    \"\"\"Build CTC model for speech recognition\"\"\"\n",
    "    \n",
    "    # Input layer\n",
    "    input_data = layers.Input(shape=(None, input_dim), name='input')\n",
    "    \n",
    "    # Convolutional layers for feature extraction\n",
    "    x = layers.Conv1D(64, 3, activation='relu', padding='same')(input_data)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Dropout(0.2)(x)\n",
    "    x = layers.MaxPooling1D(2)(x)\n",
    "    \n",
    "    x = layers.Conv1D(128, 3, activation='relu', padding='same')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Dropout(0.2)(x)\n",
    "    \n",
    "    # Recurrent layers\n",
    "    x = layers.Bidirectional(layers.LSTM(rnn_units, return_sequences=True, dropout=0.2))(x)\n",
    "    x = layers.Bidirectional(layers.LSTM(rnn_units, return_sequences=True, dropout=0.2))(x)\n",
    "    \n",
    "    # Dense layer before output\n",
    "    x = layers.Dense(256, activation='relu')(x)\n",
    "    x = layers.Dropout(0.2)(x)\n",
    "    \n",
    "    # Output layer (characters + blank token)\n",
    "    output = layers.Dense(output_dim, activation='softmax', name='output')(x)\n",
    "    \n",
    "    model = models.Model(inputs=input_data, outputs=output)\n",
    "    \n",
    "    # Custom CTC loss function\n",
    "    def ctc_loss_func(y_true, y_pred):\n",
    "        batch_len = tf.cast(tf.shape(y_true)[0], dtype=\"int64\")\n",
    "        input_length = tf.cast(tf.shape(y_pred)[1], dtype=\"int64\")\n",
    "        label_length = tf.cast(tf.shape(y_true)[1], dtype=\"int64\")\n",
    "        \n",
    "        input_length = input_length * tf.ones(shape=(batch_len, 1), dtype=\"int64\")\n",
    "        label_length = label_length * tf.ones(shape=(batch_len, 1), dtype=\"int64\")\n",
    "        \n",
    "        loss = tf.keras.backend.ctc_batch_cost(y_true, y_pred, input_length, label_length)\n",
    "        return loss\n",
    "    \n",
    "    # Compile model\n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
    "        loss=ctc_loss_func,\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "a738ed8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "Loaded 5364 entries from final_txt_data.json\n",
      "Creating vocabulary...\n",
      "Vocabulary size: 87 characters\n",
      "Characters: [' ', '!', \"'\", '(', ')', ',', '-', '.', ':', ';', '?', 'ං', 'ඃ', 'අ', 'ආ', 'ඇ', 'ඈ', 'ඉ', 'ඊ', 'උ']...\n",
      "Preparing dataset...\n",
      "Processing 100 audio files...\n",
      "Processed 0/100 files...\n",
      "Successfully processed 19 files\n",
      "Skipped 81 files\n",
      "Final shapes: X=(19, 100, 13), y=(19, 50)\n",
      "\n",
      "First few skipped files:\n",
      "  1: Text too long: 60 > 50\n",
      "  2: Text too long: 52 > 50\n",
      "  3: Text too long: 61 > 50\n",
      "  4: Text too long: 137 > 50\n",
      "  5: Text too long: 62 > 50\n",
      "Dataset shapes: X=(19, 100, 13), y=(19, 50)\n",
      "Splitting dataset...\n",
      "Training set: 15 samples\n",
      "Validation set: 4 samples\n",
      "Building model with input_dim=13, output_dim=87\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_3\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional_3\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">13</span>)       │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)       │         <span style=\"color: #00af00; text-decoration-color: #00af00\">2,560</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_4           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)       │           <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)       │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling1d_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling1D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)       │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)      │        <span style=\"color: #00af00; text-decoration-color: #00af00\">24,704</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_5           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)      │           <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ bidirectional_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Bidirectional</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)      │       <span style=\"color: #00af00; text-decoration-color: #00af00\">263,168</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ bidirectional_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Bidirectional</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)      │       <span style=\"color: #00af00; text-decoration-color: #00af00\">394,240</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)      │        <span style=\"color: #00af00; text-decoration-color: #00af00\">65,792</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ output (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">87</span>)       │        <span style=\"color: #00af00; text-decoration-color: #00af00\">22,359</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input (\u001b[38;5;33mInputLayer\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m13\u001b[0m)       │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_4 (\u001b[38;5;33mConv1D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)       │         \u001b[38;5;34m2,560\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_4           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)       │           \u001b[38;5;34m256\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_3 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)       │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling1d_3 (\u001b[38;5;33mMaxPooling1D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)       │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_5 (\u001b[38;5;33mConv1D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)      │        \u001b[38;5;34m24,704\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_5           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)      │           \u001b[38;5;34m512\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_4 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)      │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ bidirectional_6 (\u001b[38;5;33mBidirectional\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)      │       \u001b[38;5;34m263,168\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ bidirectional_7 (\u001b[38;5;33mBidirectional\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)      │       \u001b[38;5;34m394,240\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_3 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)      │        \u001b[38;5;34m65,792\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_5 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)      │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ output (\u001b[38;5;33mDense\u001b[0m)                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m87\u001b[0m)       │        \u001b[38;5;34m22,359\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">773,591</span> (2.95 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m773,591\u001b[0m (2.95 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">773,207</span> (2.95 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m773,207\u001b[0m (2.95 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">384</span> (1.50 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m384\u001b[0m (1.50 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n",
      "Epoch 1/20\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 1s/step - accuracy: 0.0182 - loss: inf - val_accuracy: 0.0200 - val_loss: inf - learning_rate: 0.0010\n",
      "Epoch 2/20\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 73ms/step - accuracy: 0.0324 - loss: inf - val_accuracy: 0.0200 - val_loss: inf - learning_rate: 0.0010\n",
      "Epoch 3/20\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 74ms/step - accuracy: 0.0514 - loss: inf - val_accuracy: 0.0200 - val_loss: inf - learning_rate: 0.0010\n",
      "Epoch 4/20\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 73ms/step - accuracy: 0.0730 - loss: inf - val_accuracy: 0.0300 - val_loss: inf - learning_rate: 5.0000e-04\n",
      "Epoch 5/20\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 72ms/step - accuracy: 0.0723 - loss: inf - val_accuracy: 0.0500 - val_loss: inf - learning_rate: 5.0000e-04\n",
      "Epoch 6/20\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 73ms/step - accuracy: 0.0912 - loss: inf - val_accuracy: 0.0700 - val_loss: inf - learning_rate: 5.0000e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model...\n",
      "Converting to TFLite...\n",
      "INFO:tensorflow:Assets written to: C:\\Users\\User\\AppData\\Local\\Temp\\tmpn5b8pbdt\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\User\\AppData\\Local\\Temp\\tmpn5b8pbdt\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved artifact at 'C:\\Users\\User\\AppData\\Local\\Temp\\tmpn5b8pbdt'. The following endpoints are available:\n",
      "\n",
      "* Endpoint 'serve'\n",
      "  args_0 (POSITIONAL_ONLY): TensorSpec(shape=(None, None, 13), dtype=tf.float32, name='input')\n",
      "Output Type:\n",
      "  TensorSpec(shape=(None, None, 87), dtype=tf.float32, name=None)\n",
      "Captures:\n",
      "  2359596581520: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2359596582864: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2359596583248: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2359596582480: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2359596582288: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2359596583056: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2359596584784: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2359596585552: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2359596585936: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2359596585168: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2359596584976: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2359596585744: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2359596587472: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2359596587664: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2359596589008: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2359596589968: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2359596588240: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2359596588816: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2359596589584: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2359596591120: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2359596591696: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2359596591888: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2359596592464: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2359596593424: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2359596590928: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2359596592272: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2359596593040: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2359277501136: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2359596595152: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2359596594384: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2359596592656: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2359596596304: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "TFLite conversion successful!\n",
      "Training completed successfully!\n",
      "Files saved:\n",
      "  - sinhala_stt_model.h5 (Keras model)\n",
      "  - sinhala_stt.tflite (TensorFlow Lite model)\n",
      "  - vocabulary.json (Character vocabulary)\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    \"\"\"Main training function\"\"\"\n",
    "    \n",
    "    # Ensure sklearn is imported (re-import to be safe)\n",
    "    try:\n",
    "        from sklearn.model_selection import train_test_split\n",
    "    except ImportError:\n",
    "        print(\"Error: sklearn not installed. Run: pip install scikit-learn\")\n",
    "        return\n",
    "    \n",
    "    # Check if data file exists\n",
    "    json_path = \"final_txt_data.json\"\n",
    "    if not os.path.exists(json_path):\n",
    "        print(f\"Error: {json_path} not found!\")\n",
    "        return\n",
    "    \n",
    "    # Check if audio directory exists\n",
    "    if not os.path.exists(AUDIO_DIR):\n",
    "        print(f\"Error: Audio directory {AUDIO_DIR} not found!\")\n",
    "        return\n",
    "    \n",
    "    print(\"Loading data...\")\n",
    "    data = load_data(json_path)\n",
    "    if not data:\n",
    "        print(\"No data loaded!\")\n",
    "        return\n",
    "    \n",
    "    print(\"Creating vocabulary...\")\n",
    "    char_to_num, num_to_char = create_vocab(data)\n",
    "    \n",
    "    print(\"Preparing dataset...\")\n",
    "    # Limit samples for testing (remove max_samples=100 for full dataset)\n",
    "    X, y = prepare_dataset(data, char_to_num, max_samples=100)\n",
    "    \n",
    "    if len(X) == 0:\n",
    "        print(\"No valid samples found. Please check your data and audio files.\")\n",
    "        return\n",
    "    \n",
    "    # Check data shapes\n",
    "    print(f\"Dataset shapes: X={X.shape}, y={y.shape}\")\n",
    "    \n",
    "    if len(X) < 10:\n",
    "        print(f\"Warning: Only {len(X)} samples found. Need more data for training.\")\n",
    "        return\n",
    "    \n",
    "    # Split dataset\n",
    "    print(\"Splitting dataset...\")\n",
    "    X_train, X_val, y_train, y_val = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=42, shuffle=True\n",
    "    )\n",
    "    \n",
    "    print(f\"Training set: {X_train.shape[0]} samples\")\n",
    "    print(f\"Validation set: {X_val.shape[0]} samples\")\n",
    "    \n",
    "    # Build model\n",
    "    input_dim = X.shape[2]  # Number of MFCC features\n",
    "    output_dim = len(char_to_num)  # Number of unique characters including blank\n",
    "    \n",
    "    print(f\"Building model with input_dim={input_dim}, output_dim={output_dim}\")\n",
    "    model = build_model(input_dim, output_dim)\n",
    "    \n",
    "    # Print model summary\n",
    "    model.summary()\n",
    "    \n",
    "    # Training callbacks\n",
    "    callbacks = [\n",
    "        tf.keras.callbacks.EarlyStopping(\n",
    "            monitor='val_loss',\n",
    "            patience=5,\n",
    "            restore_best_weights=True\n",
    "        ),\n",
    "        tf.keras.callbacks.ReduceLROnPlateau(\n",
    "            monitor='val_loss',\n",
    "            factor=0.5,\n",
    "            patience=3,\n",
    "            min_lr=0.00001\n",
    "        )\n",
    "    ]\n",
    "    \n",
    "    # Train model\n",
    "    print(\"Starting training...\")\n",
    "    try:\n",
    "        history = model.fit(\n",
    "            X_train, y_train,\n",
    "            validation_data=(X_val, y_val),\n",
    "            epochs=20,\n",
    "            batch_size=8,\n",
    "            callbacks=callbacks,\n",
    "            verbose=1\n",
    "        )\n",
    "        \n",
    "        # Save model\n",
    "        print(\"Saving model...\")\n",
    "        model.save('sinhala_stt_model.h5')\n",
    "        \n",
    "        # Convert to TFLite (with fixes for LSTM compatibility)\n",
    "        print(\"Converting to TFLite...\")\n",
    "        try:\n",
    "            converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "            \n",
    "            # Fix for LSTM/RNN conversion issues\n",
    "            converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "            converter.target_spec.supported_ops = [\n",
    "                tf.lite.OpsSet.TFLITE_BUILTINS,\n",
    "                tf.lite.OpsSet.SELECT_TF_OPS  # Allow TensorFlow ops\n",
    "            ]\n",
    "            converter._experimental_lower_tensor_list_ops = False\n",
    "            converter.experimental_enable_resource_variables = True\n",
    "            \n",
    "            tflite_model = converter.convert()\n",
    "            \n",
    "            with open('sinhala_stt.tflite', 'wb') as f:\n",
    "                f.write(tflite_model)\n",
    "            \n",
    "            print(\"TFLite conversion successful!\")\n",
    "            \n",
    "        except Exception as tflite_error:\n",
    "            print(f\"TFLite conversion failed: {tflite_error}\")\n",
    "            print(\"Saving model in SavedModel format instead...\")\n",
    "            \n",
    "            # Alternative: Save as SavedModel format\n",
    "            model.save('sinhala_stt_savedmodel', save_format='tf')\n",
    "            print(\"Model saved as SavedModel format (sinhala_stt_savedmodel/)\")\n",
    "        \n",
    "        # Save vocabulary\n",
    "        vocab_data = {\n",
    "            'char_to_num': char_to_num,\n",
    "            'num_to_char': num_to_char\n",
    "        }\n",
    "        with open('vocabulary.json', 'w', encoding='utf-8') as f:\n",
    "            json.dump(vocab_data, f, ensure_ascii=False, indent=2)\n",
    "        \n",
    "        print(\"Training completed successfully!\")\n",
    "        print(\"Files saved:\")\n",
    "        print(\"  - sinhala_stt_model.h5 (Keras model)\")\n",
    "        if os.path.exists('sinhala_stt.tflite'):\n",
    "            print(\"  - sinhala_stt.tflite (TensorFlow Lite model)\")\n",
    "        if os.path.exists('sinhala_stt_savedmodel'):\n",
    "            print(\"  - sinhala_stt_savedmodel/ (SavedModel format)\")\n",
    "        print(\"  - vocabulary.json (Character vocabulary)\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Training failed: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65f77344",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
