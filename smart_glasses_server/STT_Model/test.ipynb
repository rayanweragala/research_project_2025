{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "54d79492",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import librosa\n",
    "import soundfile as sf\n",
    "import pickle\n",
    "from typing import List, Optional\n",
    "import matplotlib.pyplot as plt\n",
    "import IPython.display as ipd\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c4bb4920",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "CONFIG = {\n",
    "    'SAMPLE_RATE': 16000,\n",
    "    'MAX_AUDIO_LENGTH': 16000 * 10,  # 10 seconds\n",
    "    'MAX_TEXT_LENGTH': 100,\n",
    "    'N_MELS': 80,\n",
    "    'FIXED_SEQUENCE_LENGTH': 625\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c9490eac",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SinhalaSTTTester:\n",
    "    \"\"\"Enhanced Testing class for Sinhala STT model with debugging\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.model = None\n",
    "        self.char_to_idx = {}\n",
    "        self.idx_to_char = {}\n",
    "        self.vocab_size = 0\n",
    "        self.interpreter = None\n",
    "        self.input_details = None\n",
    "        self.output_details = None\n",
    "        \n",
    "        # Create basic vocabulary\n",
    "        self.create_basic_vocabulary()\n",
    "    \n",
    "    def create_basic_vocabulary(self):\n",
    "        \"\"\"Create a basic Sinhala vocabulary for testing\"\"\"\n",
    "        print(\"Creating basic Sinhala vocabulary...\")\n",
    "        \n",
    "        # Basic Sinhala characters and common words\n",
    "        sinhala_chars = [\n",
    "            '‡∂Ö', '‡∂Ü', '‡∂á', '‡∂à', '‡∂â', '‡∂ä', '‡∂ã', '‡∂å', '‡∂ç', '‡∂é', '‡∂è', '‡∂ê', '‡∂ë', '‡∂í', '‡∂ì', '‡∂î', '‡∂ï', '‡∂ñ',\n",
    "            '‡∂ö', '‡∂õ', '‡∂ú', '‡∂ù', '‡∂û', '‡∂†', '‡∂°', '‡∂¢', '‡∂£', '‡∂§', '‡∂ß', '‡∂®', '‡∂©', '‡∂™', '‡∂´', '‡∂≠', '‡∂Æ', '‡∂Ø', '‡∂∞', '‡∂±',\n",
    "            '‡∂¥', '‡∂µ', '‡∂∂', '‡∂∑', '‡∂∏', '‡∂∫', '‡∂ª', '‡∂Ω', '‡∑Ä', '‡∑Å', '‡∑Ç', '‡∑É', '‡∑Ñ', '‡∑Ö', '‡∑Ü',\n",
    "            '‡∑è', '‡∑í', '‡∑ì', '‡∑î', '‡∑ñ', '‡∑ò', '‡∑ô', '‡∑ö', '‡∑õ', '‡∑ú', '‡∑ù', '‡∑û', '‡∂Ç', '‡∂É', '‡∑ä',\n",
    "            ' ', '.', ',', '?', '!', '-', ':', ';', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9'\n",
    "        ]\n",
    "        \n",
    "        # Add special tokens\n",
    "        special_tokens = ['<pad>', '<sos>', '<eos>', '<unk>']\n",
    "        vocab = special_tokens + sinhala_chars\n",
    "        \n",
    "        self.char_to_idx = {char: idx for idx, char in enumerate(vocab)}\n",
    "        self.idx_to_char = {idx: char for char, idx in self.char_to_idx.items()}\n",
    "        self.vocab_size = len(vocab)\n",
    "        \n",
    "        print(f\"‚úì Basic vocabulary created with {self.vocab_size} characters\")\n",
    "        print(f\"‚úì Special tokens: {special_tokens}\")\n",
    "        print(f\"‚úì SOS token index: {self.char_to_idx['<sos>']}\")\n",
    "        print(f\"‚úì EOS token index: {self.char_to_idx['<eos>']}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "974ea0e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "    def load_model(self, model_path: str):\n",
    "        \"\"\"Load the trained model with enhanced debugging\"\"\"\n",
    "        try:\n",
    "            print(f\"\\nüîß Loading model from: {model_path}\")\n",
    "            print(f\"üìÅ File exists: {os.path.exists(model_path)}\")\n",
    "            print(f\"üìä File size: {os.path.getsize(model_path) / 1024 / 1024:.2f} MB\")\n",
    "            \n",
    "            if model_path.endswith('.tflite'):\n",
    "                # Load TFLite model\n",
    "                print(\"üöÄ Loading TFLite model...\")\n",
    "                self.interpreter = tf.lite.Interpreter(model_path=model_path)\n",
    "                self.interpreter.allocate_tensors()\n",
    "                self.input_details = self.interpreter.get_input_details()\n",
    "                self.output_details = self.interpreter.get_output_details()\n",
    "                \n",
    "                print(\"‚úì TFLite model loaded successfully\")\n",
    "                print(f\"üìä Input details:\")\n",
    "                for i, detail in enumerate(self.input_details):\n",
    "                    print(f\"   Input {i}: {detail['name']} - Shape: {detail['shape']} - Type: {detail['dtype']}\")\n",
    "                print(f\"üìä Output details:\")\n",
    "                for i, detail in enumerate(self.output_details):\n",
    "                    print(f\"   Output {i}: {detail['name']} - Shape: {detail['shape']} - Type: {detail['dtype']}\")\n",
    "                    \n",
    "            else:\n",
    "                # Load Keras model\n",
    "                print(\"üöÄ Loading Keras model...\")\n",
    "                self.model = keras.models.load_model(model_path)\n",
    "                print(\"‚úì Keras model loaded successfully\")\n",
    "                \n",
    "                # Print model structure\n",
    "                print(\"\\nüìä Model Architecture:\")\n",
    "                self.model.summary()\n",
    "                \n",
    "                # Check input/output shapes\n",
    "                print(f\"\\nüìä Model Input Shape: {self.model.input_shape}\")\n",
    "                print(f\"üìä Model Output Shape: {self.model.output_shape}\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error loading model: {e}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "            self.model = None\n",
    "            return False\n",
    "        return True\n",
    "    \n",
    "    def load_vocabulary(self, vocab_path: str):\n",
    "        \"\"\"Load vocabulary from file with debugging\"\"\"\n",
    "        try:\n",
    "            print(f\"\\nüîß Loading vocabulary from: {vocab_path}\")\n",
    "            print(f\"üìÅ File exists: {os.path.exists(vocab_path)}\")\n",
    "            \n",
    "            with open(vocab_path, 'rb') as f:\n",
    "                vocab_data = pickle.load(f)\n",
    "            \n",
    "            print(f\"üìä Vocabulary file contents: {list(vocab_data.keys())}\")\n",
    "            \n",
    "            self.char_to_idx = vocab_data['char_to_idx']\n",
    "            self.idx_to_char = vocab_data['idx_to_char']\n",
    "            self.vocab_size = vocab_data['vocab_size']\n",
    "            \n",
    "            print(f\"‚úì Vocabulary loaded: {self.vocab_size} characters\")\n",
    "            print(f\"‚úì Sample characters: {list(self.char_to_idx.keys())[:10]}\")\n",
    "            return True\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error loading vocabulary: {e}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "            return False\n",
    "    \n",
    "    def display_audio_info(self, audio_path: str):\n",
    "        \"\"\"Display audio information and waveform\"\"\"\n",
    "        try:\n",
    "            # Load audio\n",
    "            audio, sr = librosa.load(audio_path, sr=None)\n",
    "            \n",
    "            print(f\"\\nüìä Audio Information:\")\n",
    "            print(f\"  - File: {os.path.basename(audio_path)}\")\n",
    "            print(f\"  - Sample rate: {sr} Hz\")\n",
    "            print(f\"  - Duration: {len(audio)/sr:.2f} seconds\")\n",
    "            print(f\"  - Samples: {len(audio)}\")\n",
    "            print(f\"  - Audio range: [{audio.min():.4f}, {audio.max():.4f}]\")\n",
    "            print(f\"  - Audio mean: {audio.mean():.4f}\")\n",
    "            print(f\"  - Audio std: {audio.std():.4f}\")\n",
    "            \n",
    "            # Check for silence\n",
    "            if np.abs(audio).max() < 0.01:\n",
    "                print(\"‚ö†Ô∏è  WARNING: Audio seems very quiet, might be silence\")\n",
    "            \n",
    "            # Display audio player\n",
    "            print(\"\\nüéµ Audio Player:\")\n",
    "            ipd.display(ipd.Audio(audio_path))\n",
    "            \n",
    "            # Plot waveform\n",
    "            plt.figure(figsize=(12, 4))\n",
    "            plt.subplot(1, 2, 1)\n",
    "            time = np.linspace(0, len(audio)/sr, len(audio))\n",
    "            plt.plot(time, audio)\n",
    "            plt.title('Waveform')\n",
    "            plt.xlabel('Time (s)')\n",
    "            plt.ylabel('Amplitude')\n",
    "            plt.grid(True)\n",
    "            \n",
    "            # Plot spectrogram\n",
    "            plt.subplot(1, 2, 2)\n",
    "            D = librosa.stft(audio)\n",
    "            S_db = librosa.amplitude_to_db(np.abs(D), ref=np.max)\n",
    "            librosa.display.specshow(S_db, sr=sr, x_axis='time', y_axis='hz')\n",
    "            plt.title('Spectrogram')\n",
    "            plt.colorbar(format='%+2.0f dB')\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error displaying audio info: {e}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "    \n",
    "    def preprocess_audio(self, audio_path: str) -> np.ndarray:\n",
    "        \"\"\"Preprocess audio file to mel spectrogram features with debugging\"\"\"\n",
    "        try:\n",
    "            print(f\"\\nüé§ Processing audio: {os.path.basename(audio_path)}\")\n",
    "            \n",
    "            # Load audio\n",
    "            audio, sr = librosa.load(audio_path, sr=CONFIG['SAMPLE_RATE'])\n",
    "            print(f\"   - Original sample rate: {sr} Hz\")\n",
    "            print(f\"   - Target sample rate: {CONFIG['SAMPLE_RATE']} Hz\")\n",
    "            print(f\"   - Original length: {len(audio)} samples ({len(audio)/CONFIG['SAMPLE_RATE']:.2f} seconds)\")\n",
    "            print(f\"   - Audio range: [{audio.min():.4f}, {audio.max():.4f}]\")\n",
    "            \n",
    "            # Check for silence\n",
    "            if np.abs(audio).max() < 0.01:\n",
    "                print(\"‚ö†Ô∏è  WARNING: Audio seems very quiet, might be silence\")\n",
    "            \n",
    "            # Pad or trim audio to fixed length\n",
    "            if len(audio) > CONFIG['MAX_AUDIO_LENGTH']:\n",
    "                audio = audio[:CONFIG['MAX_AUDIO_LENGTH']]\n",
    "                print(f\"   - Audio trimmed to {CONFIG['MAX_AUDIO_LENGTH']} samples\")\n",
    "            else:\n",
    "                audio = np.pad(audio, (0, CONFIG['MAX_AUDIO_LENGTH'] - len(audio)), 'constant')\n",
    "                print(f\"   - Audio padded to {CONFIG['MAX_AUDIO_LENGTH']} samples\")\n",
    "            \n",
    "            # Extract mel spectrogram\n",
    "            print(f\"   - Extracting mel spectrogram with {CONFIG['N_MELS']} mel bands\")\n",
    "            mel_spec = librosa.feature.melspectrogram(\n",
    "                y=audio, \n",
    "                sr=CONFIG['SAMPLE_RATE'],\n",
    "                n_mels=CONFIG['N_MELS'],\n",
    "                n_fft=1024,\n",
    "                hop_length=256,\n",
    "                win_length=1024\n",
    "            )\n",
    "            \n",
    "            print(f\"   - Mel spectrogram shape before processing: {mel_spec.shape}\")\n",
    "            \n",
    "            # Convert to log scale and normalize\n",
    "            mel_spec = librosa.power_to_db(mel_spec, ref=np.max)\n",
    "            print(f\"   - Mel spec range after log: [{mel_spec.min():.2f}, {mel_spec.max():.2f}]\")\n",
    "            \n",
    "            mel_spec = (mel_spec - np.mean(mel_spec)) / (np.std(mel_spec) + 1e-8)\n",
    "            print(f\"   - Mel spec range after normalization: [{mel_spec.min():.2f}, {mel_spec.max():.2f}]\")\n",
    "            \n",
    "            # Transpose and ensure fixed dimensions\n",
    "            mel_spec = mel_spec.T  # Shape: (time_steps, n_mels)\n",
    "            print(f\"   - Mel spec shape after transpose: {mel_spec.shape}\")\n",
    "            \n",
    "            # Fixed sequence length for compatibility\n",
    "            if mel_spec.shape[0] > CONFIG['FIXED_SEQUENCE_LENGTH']:\n",
    "                mel_spec = mel_spec[:CONFIG['FIXED_SEQUENCE_LENGTH'], :]\n",
    "                print(f\"   - Mel spec trimmed to {CONFIG['FIXED_SEQUENCE_LENGTH']} time steps\")\n",
    "            else:\n",
    "                pad_length = CONFIG['FIXED_SEQUENCE_LENGTH'] - mel_spec.shape[0]\n",
    "                mel_spec = np.pad(mel_spec, ((0, pad_length), (0, 0)), 'constant')\n",
    "                print(f\"   - Mel spec padded to {CONFIG['FIXED_SEQUENCE_LENGTH']} time steps\")\n",
    "            \n",
    "            print(f\"   - Final mel spectrogram shape: {mel_spec.shape}\")\n",
    "            \n",
    "            # Check for NaN or Inf values\n",
    "            if np.isnan(mel_spec).any():\n",
    "                print(\"‚ö†Ô∏è  WARNING: NaN values detected in mel spectrogram\")\n",
    "            if np.isinf(mel_spec).any():\n",
    "                print(\"‚ö†Ô∏è  WARNING: Inf values detected in mel spectrogram\")\n",
    "            \n",
    "            # Visualize mel spectrogram\n",
    "            plt.figure(figsize=(12, 6))\n",
    "            librosa.display.specshow(mel_spec.T, sr=CONFIG['SAMPLE_RATE'], \n",
    "                                   x_axis='time', y_axis='mel', cmap='viridis')\n",
    "            plt.title('Mel Spectrogram Features')\n",
    "            plt.colorbar(format='%+2.0f dB')\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "            \n",
    "            return mel_spec\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error processing audio: {e}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "            return np.zeros((CONFIG['FIXED_SEQUENCE_LENGTH'], CONFIG['N_MELS']))\n",
    "    \n",
    "    def sequence_to_text(self, sequence: List[int]) -> str:\n",
    "        \"\"\"Convert sequence of indices back to text with debugging\"\"\"\n",
    "        text = \"\"\n",
    "        print(f\"üî§ Converting sequence to text: {sequence[:20]}...\")  # Show first 20 indices\n",
    "        \n",
    "        for i, idx in enumerate(sequence):\n",
    "            if idx in self.idx_to_char:\n",
    "                char = self.idx_to_char[idx]\n",
    "                if char in ['<pad>', '<sos>', '<eos>']:\n",
    "                    if char == '<eos>':\n",
    "                        print(f\"   - Found EOS token at position {i}\")\n",
    "                        break\n",
    "                    continue\n",
    "                elif char == '<unk>':\n",
    "                    text += '?'\n",
    "                    print(f\"   - Found UNK token at position {i}\")\n",
    "                else:\n",
    "                    text += char\n",
    "            else:\n",
    "                print(f\"   - Unknown index {idx} at position {i}\")\n",
    "                text += '?'\n",
    "        \n",
    "        print(f\"   - Final text length: {len(text)}\")\n",
    "        return text\n",
    "    \n",
    "    def greedy_decode(self, audio_features: np.ndarray) -> str:\n",
    "        \"\"\"Simple greedy decoding with enhanced debugging\"\"\"\n",
    "        if self.model is None and self.interpreter is None:\n",
    "            print(\"‚ùå No model loaded\")\n",
    "            return \"\"\n",
    "        \n",
    "        print(\"\\nüìù Using greedy decoding...\")\n",
    "        \n",
    "        # Prepare inputs\n",
    "        audio_input = np.expand_dims(audio_features, axis=0).astype(np.float32)\n",
    "        print(f\"   - Audio input shape: {audio_input.shape}\")\n",
    "        \n",
    "        if self.interpreter is not None:\n",
    "            # TFLite inference\n",
    "            print(\"üöÄ Running TFLite inference...\")\n",
    "            decoder_input = np.zeros((1, CONFIG['MAX_TEXT_LENGTH'] - 1), dtype=np.int32)\n",
    "            decoder_input[0, 0] = self.char_to_idx['<sos>']\n",
    "            \n",
    "            print(f\"   - Decoder input shape: {decoder_input.shape}\")\n",
    "            print(f\"   - SOS token: {self.char_to_idx['<sos>']}\")\n",
    "            \n",
    "            # Set inputs\n",
    "            self.interpreter.set_tensor(self.input_details[0]['index'], audio_input)\n",
    "            self.interpreter.set_tensor(self.input_details[1]['index'], decoder_input)\n",
    "            \n",
    "            # Run inference\n",
    "            self.interpreter.invoke()\n",
    "            \n",
    "            # Get output\n",
    "            prediction = self.interpreter.get_tensor(self.output_details[0]['index'])\n",
    "            print(f\"   - Prediction shape: {prediction.shape}\")\n",
    "            print(f\"   - Prediction range: [{prediction.min():.4f}, {prediction.max():.4f}]\")\n",
    "            \n",
    "            # Get predicted sequence\n",
    "            predicted_sequence = np.argmax(prediction[0], axis=-1)\n",
    "            print(f\"   - Predicted sequence: {predicted_sequence[:20]}...\")\n",
    "            \n",
    "            predicted_text = self.sequence_to_text(predicted_sequence)\n",
    "            \n",
    "        else:\n",
    "            # Keras model inference\n",
    "            print(\"üöÄ Running Keras model inference...\")\n",
    "            decoder_input = np.zeros((1, CONFIG['MAX_TEXT_LENGTH'] - 1))\n",
    "            decoder_input[0, 0] = self.char_to_idx['<sos>']\n",
    "            \n",
    "            print(f\"   - Decoder input shape: {decoder_input.shape}\")\n",
    "            print(f\"   - SOS token: {self.char_to_idx['<sos>']}\")\n",
    "            \n",
    "            predicted_text = \"\"\n",
    "            \n",
    "            for i in range(1, CONFIG['MAX_TEXT_LENGTH'] - 1):\n",
    "                # Get prediction\n",
    "                prediction = self.model.predict([audio_input, decoder_input], verbose=0)\n",
    "                print(f\"   - Step {i}: Prediction shape: {prediction.shape}\")\n",
    "                \n",
    "                # Get next token\n",
    "                next_token = np.argmax(prediction[0, i-1, :])\n",
    "                print(f\"   - Step {i}: Next token: {next_token} ({self.idx_to_char.get(next_token, 'UNK')})\")\n",
    "                \n",
    "                # Stop if EOS token\n",
    "                if next_token == self.char_to_idx['<eos>']:\n",
    "                    print(f\"   - EOS token found at step {i}\")\n",
    "                    break\n",
    "                \n",
    "                # Add to decoder input for next prediction\n",
    "                decoder_input[0, i] = next_token\n",
    "            \n",
    "            # Convert to text\n",
    "            predicted_sequence = decoder_input[0, 1:i+1]  # Skip SOS token\n",
    "            predicted_text = self.sequence_to_text(predicted_sequence.astype(int))\n",
    "        \n",
    "        print(f\"   - Final predicted text: '{predicted_text}'\")\n",
    "        return predicted_text\n",
    "    \n",
    "    def beam_search_decode(self, audio_features: np.ndarray, beam_width: int = 3) -> str:\n",
    "        \"\"\"Beam search decoding for better results with debugging\"\"\"\n",
    "        if self.model is None:\n",
    "            print(\"‚ùå Beam search only supported for Keras models\")\n",
    "            return self.greedy_decode(audio_features)\n",
    "        \n",
    "        print(f\"\\nüîç Using beam search decoding (beam_width={beam_width})...\")\n",
    "        \n",
    "        audio_input = np.expand_dims(audio_features, axis=0)\n",
    "        \n",
    "        # Initialize beam\n",
    "        beams = [(0.0, [self.char_to_idx['<sos>']])]\n",
    "        print(f\"   - Initial beam: {beams}\")\n",
    "        \n",
    "        for step in range(CONFIG['MAX_TEXT_LENGTH'] - 1):\n",
    "            print(f\"   - Step {step}: Processing {len(beams)} beams\")\n",
    "            new_beams = []\n",
    "            \n",
    "            for beam_idx, (score, sequence) in enumerate(beams):\n",
    "                if len(sequence) > 0 and sequence[-1] == self.char_to_idx['<eos>']:\n",
    "                    new_beams.append((score, sequence))\n",
    "                    continue\n",
    "                \n",
    "                # Prepare decoder input\n",
    "                decoder_input = np.zeros((1, CONFIG['MAX_TEXT_LENGTH'] - 1))\n",
    "                for i, token in enumerate(sequence):\n",
    "                    if i < CONFIG['MAX_TEXT_LENGTH'] - 1:\n",
    "                        decoder_input[0, i] = token\n",
    "                \n",
    "                # Get predictions\n",
    "                predictions = self.model.predict([audio_input, decoder_input], verbose=0)\n",
    "                \n",
    "                # Get probabilities for next token\n",
    "                if len(sequence) - 1 < predictions.shape[1]:\n",
    "                    next_token_probs = predictions[0, len(sequence) - 1, :]\n",
    "                    \n",
    "                    # Get top beam_width predictions\n",
    "                    top_indices = np.argsort(next_token_probs)[-beam_width:]\n",
    "                    \n",
    "                    for idx in top_indices:\n",
    "                        prob = next_token_probs[idx]\n",
    "                        if prob > 1e-10:  # Avoid log(0)\n",
    "                            new_score = score + np.log(prob)\n",
    "                            new_sequence = sequence + [idx]\n",
    "                            new_beams.append((new_score, new_sequence))\n",
    "            \n",
    "            if not new_beams:\n",
    "                print(f\"   - No valid beams at step {step}\")\n",
    "                break\n",
    "            \n",
    "            # Keep only top beam_width beams\n",
    "            beams = sorted(new_beams, key=lambda x: x[0], reverse=True)[:beam_width]\n",
    "            \n",
    "            # Stop if all beams ended\n",
    "            if all(len(seq) > 0 and seq[-1] == self.char_to_idx['<eos>'] for _, seq in beams):\n",
    "                print(f\"   - All beams ended at step {step}\")\n",
    "                break\n",
    "        \n",
    "        # Return best sequence\n",
    "        if beams:\n",
    "            best_score, best_sequence = beams[0]\n",
    "            print(f\"   - Best beam score: {best_score}\")\n",
    "            print(f\"   - Best sequence: {best_sequence}\")\n",
    "            result = self.sequence_to_text(best_sequence[1:])  # Skip SOS token\n",
    "            return result\n",
    "        else:\n",
    "            print(\"   - No valid beams found\")\n",
    "            return \"\"\n",
    "    \n",
    "    def test_audio(self, audio_path: str, use_beam_search: bool = True, beam_width: int = 3, \n",
    "                   show_audio_info: bool = True) -> str:\n",
    "        \"\"\"Test audio file with the model - Enhanced with debugging\"\"\"\n",
    "        if not os.path.exists(audio_path):\n",
    "            print(f\"‚ùå Audio file not found: {audio_path}\")\n",
    "            return \"\"\n",
    "        \n",
    "        print(f\"\\nüéØ Testing Sinhala STT Model\")\n",
    "        print(\"=\" * 80)\n",
    "        \n",
    "        # Display audio information\n",
    "        if show_audio_info:\n",
    "            self.display_audio_info(audio_path)\n",
    "        \n",
    "        # Preprocess audio\n",
    "        audio_features = self.preprocess_audio(audio_path)\n",
    "        \n",
    "        if self.model is None and self.interpreter is None:\n",
    "            print(\"‚ùå No model loaded. Cannot perform inference.\")\n",
    "            return \"\"\n",
    "        \n",
    "        # Perform inference\n",
    "        try:\n",
    "            if use_beam_search and self.model is not None:\n",
    "                predicted_text = self.beam_search_decode(audio_features, beam_width)\n",
    "            else:\n",
    "                predicted_text = self.greedy_decode(audio_features)\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error during inference: {e}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "            return \"\"\n",
    "        \n",
    "        # Display results\n",
    "        print(f\"\\nüéâ FINAL RESULTS:\")\n",
    "        print(\"=\" * 80)\n",
    "        print(f\"üìÅ Audio File: {os.path.basename(audio_path)}\")\n",
    "        print(f\"üî§ Predicted Text: '{predicted_text}'\")\n",
    "        print(f\"üìè Text Length: {len(predicted_text)} characters\")\n",
    "        print(f\"üìä Method: {'Beam Search' if use_beam_search and self.model else 'Greedy Decoding'}\")\n",
    "        if use_beam_search and self.model:\n",
    "            print(f\"üîç Beam Width: {beam_width}\")\n",
    "        print(\"=\" * 80)\n",
    "        \n",
    "        # Additional debugging if text is empty\n",
    "        if not predicted_text.strip():\n",
    "            print(\"‚ö†Ô∏è  WARNING: Empty transcription result!\")\n",
    "            print(\"   Possible issues:\")\n",
    "            print(\"   - Audio file might be silent or too quiet\")\n",
    "            print(\"   - Model might not be properly trained\")\n",
    "            print(\"   - Vocabulary mismatch between training and testing\")\n",
    "            print(\"   - Audio preprocessing might have issues\")\n",
    "            print(\"   - Model architecture mismatch\")\n",
    "        \n",
    "        return predicted_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f8b3e36a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    \"\"\"Enhanced main function with debugging\"\"\"\n",
    "    \n",
    "    # üîß CONFIGURATION - MODIFY THESE PATHS\n",
    "    MODEL_PATH = \"sinhala_stt_model.tflite\"  # or \"path/to/your/model.tflite\"\n",
    "    # Optional\n",
    "    AUDIO_PATH = \"converted.wav\"  # Your test audio file\n",
    "    \n",
    "    # üéõÔ∏è SETTINGS\n",
    "    USE_BEAM_SEARCH = True  # Set to False for faster greedy decoding\n",
    "    BEAM_WIDTH = 3  # Only used if USE_BEAM_SEARCH is True\n",
    "    SHOW_AUDIO_INFO = True  # Set to False to skip audio visualization\n",
    "    \n",
    "    print(\"üöÄ Initializing Enhanced Sinhala STT Tester...\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # Initialize tester\n",
    "    tester = SinhalaSTTTester()\n",
    "    \n",
    "    # Check file paths\n",
    "    print(f\"\\nüìÇ Checking file paths:\")\n",
    "    print(f\"   - Model path: {MODEL_PATH}\")\n",
    "    print(f\"   - Model exists: {os.path.exists(MODEL_PATH) if MODEL_PATH != 'path/to/your/model.h5' else 'Please set MODEL_PATH'}\")\n",
    "    print(f\"   - Vocab path: {VOCAB_PATH}\")\n",
    "    print(f\"   - Vocab exists: {os.path.exists(VOCAB_PATH) if VOCAB_PATH != 'path/to/your/vocabulary.pkl' else 'Please set VOCAB_PATH'}\")\n",
    "    print(f\"   - Audio path: {AUDIO_PATH}\")\n",
    "    print(f\"   - Audio exists: {os.path.exists(AUDIO_PATH) if AUDIO_PATH != 'path/to/your/audio.wav' else 'Please set AUDIO_PATH'}\")\n",
    "    \n",
    "    # Load vocabulary (optional)\n",
    "    if VOCAB_PATH and os.path.exists(VOCAB_PATH):\n",
    "        success = tester.load_vocabulary(VOCAB_PATH)\n",
    "        if not success:\n",
    "            print(\"‚ö†Ô∏è  Warning: Failed to load vocabulary, using basic vocabulary\")\n",
    "    else:\n",
    "        print(\"‚ÑπÔ∏è  Using basic vocabulary (vocabulary file not provided)\")\n",
    "    \n",
    "    # Load model\n",
    "    if MODEL_PATH and os.path.exists(MODEL_PATH):\n",
    "        success = tester.load_model(MODEL_PATH)\n",
    "        if not success:\n",
    "            print(\"‚ùå Failed to load model. Exiting.\")\n",
    "            return\n",
    "    else:\n",
    "        print(f\"‚ùå Model file not found: {MODEL_PATH}\")\n",
    "        print(\"Please update the MODEL_PATH variable with the correct path.\")\n",
    "        return\n",
    "    \n",
    "    # Test audio\n",
    "    if AUDIO_PATH and os.path.exists(AUDIO_PATH):\n",
    "        result = tester.test_audio(\n",
    "            audio_path=AUDIO_PATH,\n",
    "            use_beam_search=USE_BEAM_SEARCH,\n",
    "            beam_width=BEAM_WIDTH,\n",
    "            show_audio_info=SHOW_AUDIO_INFO\n",
    "        )\n",
    "        \n",
    "        if result.strip():\n",
    "            print(f\"\\n‚úÖ SUCCESS: Transcription obtained!\")\n",
    "            print(f\"Final result: '{result}'\")\n",
    "        else:\n",
    "            print(f\"\\n‚ùå ISSUE: Empty transcription result\")\n",
    "            print(\"Check the debug output above for potential issues\")\n",
    "        \n",
    "    else:\n",
    "        print(f\"‚ùå Audio file not found: {AUDIO_PATH}\")\n",
    "        print(\"Please update the AUDIO_PATH variable with the correct path.\")\n",
    "        return\n",
    "    \n",
    "    print(\"\\nüèÅ Testing completed!\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "afde1559",
   "metadata": {},
   "outputs": [],
   "source": [
    "def quick_debug():\n",
    "    \"\"\"Quick debugging function to test individual components\"\"\"\n",
    "    \n",
    "    print(\"üîç Quick Debug Mode\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Test 1: Check TensorFlow installation\n",
    "    print(f\"1. TensorFlow version: {tf.__version__}\")\n",
    "    \n",
    "    # Test 2: Check audio libraries\n",
    "    try:\n",
    "        import librosa\n",
    "        print(f\"2. Librosa version: {librosa.__version__}\")\n",
    "    except ImportError:\n",
    "        print(\"2. ‚ùå Librosa not installed\")\n",
    "    \n",
    "    # Test 3: Basic vocabulary creation\n",
    "    tester = SinhalaSTTTester()\n",
    "    print(f\"3. Vocabulary size: {tester.vocab_size}\")\n",
    "    \n",
    "    # Test 4: Audio file check (if path provided)\n",
    "    audio_path = \"path/to/your/audio.wav\"  # Update this\n",
    "    if os.path.exists(audio_path):\n",
    "        print(f\"4. Audio file found: {audio_path}\")\n",
    "        tester.display_audio_info(audio_path)\n",
    "    else:\n",
    "        print(f\"4. Audio file not found: {audio_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "52c33672",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current directory: c:\\Users\\User\\Desktop\\notbook stt mode\n",
      "Files in directory: ['audio_augmented', 'best_sinhala_stt_model.keras', 'converted.wav', 'converted_audio_new', 'file.ipynb', 'final_txt_data.json', 'nimal_hotel.mp3', 'sinhala_stt_model.h5', 'sinhala_stt_model.keras', 'sinhala_stt_model.tflite', 'sinhala_stt_processor.pkl', 'test.ipynb', 'train.ipynb']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(\"Current directory:\", os.getcwd())\n",
    "print(\"Files in directory:\", os.listdir('.'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6a0a352a",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Model provided has model identifier '\r\n\u001a\n', should be 'TFL3'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 57\u001b[0m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;66;03m# === RUN ===\u001b[39;00m\n\u001b[0;32m     56\u001b[0m mfcc_input \u001b[38;5;241m=\u001b[39m preprocess_audio_to_mfcc_1d(AUDIO_PATH)\n\u001b[1;32m---> 57\u001b[0m interpreter \u001b[38;5;241m=\u001b[39m load_tflite_model(MODEL_PATH)\n\u001b[0;32m     58\u001b[0m output \u001b[38;5;241m=\u001b[39m run_inference(interpreter, mfcc_input)\n\u001b[0;32m     59\u001b[0m transcription \u001b[38;5;241m=\u001b[39m decode_output(output, index_to_char)\n",
      "Cell \u001b[1;32mIn[6], line 30\u001b[0m, in \u001b[0;36mload_tflite_model\u001b[1;34m(model_path)\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_tflite_model\u001b[39m(model_path):\n\u001b[1;32m---> 30\u001b[0m     interpreter \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mlite\u001b[38;5;241m.\u001b[39mInterpreter(model_path\u001b[38;5;241m=\u001b[39mmodel_path)\n\u001b[0;32m     31\u001b[0m     interpreter\u001b[38;5;241m.\u001b[39mallocate_tensors()\n\u001b[0;32m     32\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m interpreter\n",
      "File \u001b[1;32mc:\\Users\\User\\anaconda3\\Lib\\site-packages\\tensorflow\\lite\\python\\interpreter.py:470\u001b[0m, in \u001b[0;36mInterpreter.__init__\u001b[1;34m(self, model_path, model_content, experimental_delegates, num_threads, experimental_op_resolver_type, experimental_preserve_all_tensors, experimental_disable_delegate_clustering, experimental_default_delegate_latest_features)\u001b[0m\n\u001b[0;32m    464\u001b[0m custom_op_registerers_by_name \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m    465\u001b[0m     x \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_custom_op_registerers \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mstr\u001b[39m)\n\u001b[0;32m    466\u001b[0m ]\n\u001b[0;32m    467\u001b[0m custom_op_registerers_by_func \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m    468\u001b[0m     x \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_custom_op_registerers \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mstr\u001b[39m)\n\u001b[0;32m    469\u001b[0m ]\n\u001b[1;32m--> 470\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_interpreter \u001b[38;5;241m=\u001b[39m _interpreter_wrapper\u001b[38;5;241m.\u001b[39mCreateWrapperFromFile(\n\u001b[0;32m    471\u001b[0m     model_path,\n\u001b[0;32m    472\u001b[0m     op_resolver_id,\n\u001b[0;32m    473\u001b[0m     custom_op_registerers_by_name,\n\u001b[0;32m    474\u001b[0m     custom_op_registerers_by_func,\n\u001b[0;32m    475\u001b[0m     experimental_preserve_all_tensors,\n\u001b[0;32m    476\u001b[0m     experimental_disable_delegate_clustering,\n\u001b[0;32m    477\u001b[0m     \u001b[38;5;28mint\u001b[39m(num_threads \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;241m1\u001b[39m),\n\u001b[0;32m    478\u001b[0m     experimental_default_delegate_latest_features,\n\u001b[0;32m    479\u001b[0m )\n\u001b[0;32m    480\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_interpreter:\n\u001b[0;32m    481\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFailed to open \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(model_path))\n",
      "\u001b[1;31mValueError\u001b[0m: Model provided has model identifier '\r\n\u001a\n', should be 'TFL3'\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import librosa\n",
    "import tensorflow as tf\n",
    "\n",
    "# === Config ===\n",
    "MODEL_PATH = \"sinhala_stt_model.h5\"\n",
    "AUDIO_PATH = \"converted.wav\"  # Replace with your .wav file\n",
    "TARGET_SR = 16000              # Sampling rate used during training\n",
    "EXPECTED_INPUT_LEN = 99        # Model expects input shape [1, 99]\n",
    "\n",
    "# === Step 1: Load and Preprocess Audio ===\n",
    "def preprocess_audio_to_mfcc_1d(audio_path, target_sr=16000, expected_len=99):\n",
    "    audio, sr = librosa.load(audio_path, sr=target_sr)\n",
    "    audio = librosa.util.normalize(audio)\n",
    "    \n",
    "    # Extract MFCCs (use only 1 coefficient per frame to match [1, 99])\n",
    "    mfcc = librosa.feature.mfcc(y=audio, sr=sr, n_mfcc=1)  # shape: [1, time]\n",
    "    \n",
    "    # Pad or trim to expected length\n",
    "    mfcc = mfcc[0]  # flatten from shape (1, time) ‚Üí (time,)\n",
    "    if len(mfcc) < expected_len:\n",
    "        mfcc = np.pad(mfcc, (0, expected_len - len(mfcc)), mode='constant')\n",
    "    else:\n",
    "        mfcc = mfcc[:expected_len]\n",
    "    \n",
    "    return mfcc[np.newaxis, :].astype(np.float32)  # shape: [1, 99]\n",
    "\n",
    "# === Step 2: Load TFLite Model ===\n",
    "def load_tflite_model(model_path):\n",
    "    interpreter = tf.lite.Interpreter(model_path=model_path)\n",
    "    interpreter.allocate_tensors()\n",
    "    return interpreter\n",
    "\n",
    "# === Step 3: Run Inference ===\n",
    "def run_inference(interpreter, input_data):\n",
    "    input_details = interpreter.get_input_details()\n",
    "    output_details = interpreter.get_output_details()\n",
    "\n",
    "    interpreter.set_tensor(input_details[0]['index'], input_data)\n",
    "    interpreter.invoke()\n",
    "\n",
    "    output_data = interpreter.get_tensor(output_details[0]['index'])\n",
    "    return output_data\n",
    "\n",
    "# === Step 4: Decode Output (Dummy Mapping) ===\n",
    "# Replace this with your actual index-to-character mapping\n",
    "index_to_char = {\n",
    "    0: '‡∂Ö', 1: '‡∂∏', 2: '‡∂ß', 3: ' ', 4: '‡∂±', 5: '‡∑í', 6: '‡∂Ω', 7: '‡∂∫', 8: '‡∑Ñ', 9: '‡∑ù', 10: ''\n",
    "}\n",
    "\n",
    "def decode_output(output, mapping):\n",
    "    output_indices = np.argmax(output, axis=-1)\n",
    "    return ''.join([mapping.get(i, '') for i in output_indices[0]])\n",
    "\n",
    "# === RUN ===\n",
    "mfcc_input = preprocess_audio_to_mfcc_1d(AUDIO_PATH)\n",
    "interpreter = load_tflite_model(MODEL_PATH)\n",
    "output = run_inference(interpreter, mfcc_input)\n",
    "transcription = decode_output(output, index_to_char)\n",
    "\n",
    "print(\"üó£Ô∏è Transcription:\", transcription)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "81022d8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected input shape: [ 1 99]\n"
     ]
    }
   ],
   "source": [
    "input_details = interpreter.get_input_details()\n",
    "print(\"Expected input shape:\", input_details[0]['shape'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad022ee5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
