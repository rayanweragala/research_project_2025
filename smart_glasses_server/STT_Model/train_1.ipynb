{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0f6b0f6",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from PIL import Image\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.regularizers import l2\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# Set matplotlib style\n",
    "plt.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8eec06a",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Define dataset paths\n",
    "base_dir = '/kaggle/input/chest-xray-pneumonia/chest_xray'  # Update this path as needed\n",
    "train_dir = os.path.join(base_dir, 'train')\n",
    "test_dir = os.path.join(base_dir, 'test')\n",
    "val_dir = os.path.join(base_dir, 'val')\n",
    "\n",
    "# Check if directories exist\n",
    "print(\"Train directory exists:\", os.path.exists(train_dir))\n",
    "print(\"Test directory exists:\", os.path.exists(test_dir))\n",
    "print(\"Val directory exists:\", os.path.exists(val_dir))\n",
    "\n",
    "# Count images in each directory\n",
    "def count_images(directory):\n",
    "    normal_count = len(os.listdir(os.path.join(directory, 'NORMAL')))\n",
    "    pneumonia_count = len(os.listdir(os.path.join(directory, 'PNEUMONIA')))\n",
    "    return normal_count, pneumonia_count\n",
    "\n",
    "train_normal, train_pneumonia = count_images(train_dir)\n",
    "test_normal, test_pneumonia = count_images(test_dir)\n",
    "val_normal, val_pneumonia = count_images(val_dir)\n",
    "\n",
    "print(f\"Training set - Normal: {train_normal}, Pneumonia: {train_pneumonia}\")\n",
    "print(f\"Test set - Normal: {test_normal}, Pneumonia: {test_pneumonia}\")\n",
    "print(f\"Validation set - Normal: {val_normal}, Pneumonia: {val_pneumonia}\")\n",
    "\n",
    "# Plot class distribution\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "sets = ['Train', 'Test', 'Validation']\n",
    "normal_counts = [train_normal, test_normal, val_normal]\n",
    "pneumonia_counts = [train_pneumonia, test_pneumonia, val_pneumonia]\n",
    "\n",
    "for i, (normal, pneumonia) in enumerate(zip(normal_counts, pneumonia_counts)):\n",
    "    axes[i].bar(['Normal', 'Pneumonia'], [normal, pneumonia], color=['skyblue', 'lightcoral'])\n",
    "    axes[i].set_title(f'{sets[i]} Set Distribution')\n",
    "    axes[i].set_ylabel('Number of Images')\n",
    "    \n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61be876f",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Display sample images from each class\n",
    "def display_sample_images(directory, title):\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(10, 5))\n",
    "    fig.suptitle(title, fontsize=16)\n",
    "    \n",
    "    # Normal sample\n",
    "    normal_img_path = os.path.join(directory, 'NORMAL', os.listdir(os.path.join(directory, 'NORMAL'))[0])\n",
    "    normal_img = Image.open(normal_img_path)\n",
    "    axes[0].imshow(normal_img, cmap='gray')\n",
    "    axes[0].set_title('Normal')\n",
    "    axes[0].axis('off')\n",
    "    \n",
    "    # Pneumonia sample\n",
    "    pneumonia_img_path = os.path.join(directory, 'PNEUMONIA', os.listdir(os.path.join(directory, 'PNEUMONIA'))[0])\n",
    "    pneumonia_img = Image.open(pneumonia_img_path)\n",
    "    axes[1].imshow(pneumonia_img, cmap='gray')\n",
    "    axes[1].set_title('Pneumonia')\n",
    "    axes[1].axis('off')\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "display_sample_images(train_dir, 'Sample Training Images')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "962f133f",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Define image dimensions\n",
    "IMG_HEIGHT = 150\n",
    "IMG_WIDTH = 150\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "# Create data generators\n",
    "# Baseline generator (only rescaling)\n",
    "baseline_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "# Strong augmentation generator\n",
    "augmented_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    rotation_range=40,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    fill_mode='nearest',\n",
    "    brightness_range=[0.8, 1.2],\n",
    "    validation_split=0.2  # Using 20% of training data for validation\n",
    ")\n",
    "\n",
    "# Create data generators\n",
    "baseline_train_generator = baseline_datagen.flow_from_directory(\n",
    "    train_dir,\n",
    "    target_size=(IMG_HEIGHT, IMG_WIDTH),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode='binary',\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "baseline_val_generator = baseline_datagen.flow_from_directory(\n",
    "    val_dir,\n",
    "    target_size=(IMG_HEIGHT, IMG_WIDTH),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode='binary',\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "# For augmented model, we'll use a subset of training data for validation\n",
    "augmented_train_generator = augmented_datagen.flow_from_directory(\n",
    "    train_dir,\n",
    "    target_size=(IMG_HEIGHT, IMG_WIDTH),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode='binary',\n",
    "    shuffle=True,\n",
    "    subset='training'\n",
    ")\n",
    "\n",
    "augmented_val_generator = augmented_datagen.flow_from_directory(\n",
    "    train_dir,\n",
    "    target_size=(IMG_HEIGHT, IMG_WIDTH),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode='binary',\n",
    "    shuffle=False,\n",
    "    subset='validation'\n",
    ")\n",
    "\n",
    "# Test generator (same for both models)\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "test_generator = test_datagen.flow_from_directory(\n",
    "    test_dir,\n",
    "    target_size=(IMG_HEIGHT, IMG_WIDTH),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode='binary',\n",
    "    shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecb33da9",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def create_cnn_model():\n",
    "    model = Sequential([\n",
    "        # First convolutional block\n",
    "        Conv2D(32, (3, 3), activation='relu', input_shape=(IMG_HEIGHT, IMG_WIDTH, 3)),\n",
    "        BatchNormalization(),\n",
    "        MaxPooling2D((2, 2)),\n",
    "        Dropout(0.25),\n",
    "        \n",
    "        # Second convolutional block\n",
    "        Conv2D(64, (3, 3), activation='relu'),\n",
    "        BatchNormalization(),\n",
    "        MaxPooling2D((2, 2)),\n",
    "        Dropout(0.25),\n",
    "        \n",
    "        # Third convolutional block\n",
    "        Conv2D(128, (3, 3), activation='relu'),\n",
    "        BatchNormalization(),\n",
    "        MaxPooling2D((2, 2)),\n",
    "        Dropout(0.25),\n",
    "        \n",
    "        # Fourth convolutional block\n",
    "        Conv2D(256, (3, 3), activation='relu'),\n",
    "        BatchNormalization(),\n",
    "        MaxPooling2D((2, 2)),\n",
    "        Dropout(0.25),\n",
    "        \n",
    "        # Flatten and dense layers\n",
    "        Flatten(),\n",
    "        Dense(512, activation='relu', kernel_regularizer=l2(0.001)),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.5),\n",
    "        Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Create baseline model\n",
    "baseline_model = create_cnn_model()\n",
    "baseline_model.compile(\n",
    "    optimizer=Adam(learning_rate=0.001),\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['accuracy', 'precision', 'recall']\n",
    ")\n",
    "\n",
    "# Create augmented model\n",
    "augmented_model = create_cnn_model()\n",
    "augmented_model.compile(\n",
    "    optimizer=Adam(learning_rate=0.001),\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['accuracy', 'precision', 'recall']\n",
    ")\n",
    "\n",
    "# Display model architecture\n",
    "baseline_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11422f84",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Define callbacks\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=10,\n",
    "    restore_best_weights=True,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "reduce_lr = ReduceLROnPlateau(\n",
    "    monitor='val_loss',\n",
    "    factor=0.2,\n",
    "    patience=5,\n",
    "    min_lr=1e-7,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Calculate class weights to handle imbalance\n",
    "total_train = train_normal + train_pneumonia\n",
    "weight_for_0 = total_train / (2 * train_normal)  # weight for normal class\n",
    "weight_for_1 = total_train / (2 * train_pneumonia)  # weight for pneumonia class\n",
    "\n",
    "class_weights = {0: weight_for_0, 1: weight_for_1}\n",
    "print(f\"Class weights: {class_weights}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe655932",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Evaluate models on test set\n",
    "print(\"Evaluating baseline model...\")\n",
    "baseline_results = baseline_model.evaluate(test_generator, verbose=0)\n",
    "print(f\"Baseline Model - Test Loss: {baseline_results[0]:.4f}, Test Accuracy: {baseline_results[1]:.4f}\")\n",
    "\n",
    "print(\"Evaluating augmented model...\")\n",
    "augmented_results = augmented_model.evaluate(test_generator, verbose=0)\n",
    "print(f\"Augmented Model - Test Loss: {augmented_results[0]:.4f}, Test Accuracy: {augmented_results[1]:.4f}\")\n",
    "\n",
    "# Plot training history comparison\n",
    "def plot_training_history(history, title):\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "    fig.suptitle(title, fontsize=16)\n",
    "    \n",
    "    # Plot accuracy\n",
    "    axes[0].plot(history.history['accuracy'], label='Training Accuracy')\n",
    "    axes[0].plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "    axes[0].set_title('Model Accuracy')\n",
    "    axes[0].set_xlabel('Epoch')\n",
    "    axes[0].set_ylabel('Accuracy')\n",
    "    axes[0].legend()\n",
    "    \n",
    "    # Plot loss\n",
    "    axes[1].plot(history.history['loss'], label='Training Loss')\n",
    "    axes[1].plot(history.history['val_loss'], label='Validation Loss')\n",
    "    axes[1].set_title('Model Loss')\n",
    "    axes[1].set_xlabel('Epoch')\n",
    "    axes[1].set_ylabel('Loss')\n",
    "    axes[1].legend()\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "plot_training_history(baseline_history, 'Baseline Model Training History')\n",
    "plot_training_history(augmented_history, 'Augmented Model Training History')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c92e89f",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Generate predictions\n",
    "baseline_predictions = baseline_model.predict(test_generator, verbose=0)\n",
    "augmented_predictions = augmented_model.predict(test_generator, verbose=0)\n",
    "\n",
    "# Convert probabilities to binary predictions\n",
    "baseline_pred_binary = (baseline_predictions > 0.5).astype(int)\n",
    "augmented_pred_binary = (augmented_predictions > 0.5).astype(int)\n",
    "\n",
    "# Get true labels\n",
    "true_labels = test_generator.classes\n",
    "\n",
    "# Generate classification reports\n",
    "print(\"Baseline Model Classification Report:\")\n",
    "print(classification_report(true_labels, baseline_pred_binary, target_names=['Normal', 'Pneumonia']))\n",
    "\n",
    "print(\"\\nAugmented Model Classification Report:\")\n",
    "print(classification_report(true_labels, augmented_pred_binary, target_names=['Normal', 'Pneumonia']))\n",
    "\n",
    "# Plot confusion matrices\n",
    "def plot_confusion_matrix(y_true, y_pred, title):\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "                xticklabels=['Normal', 'Pneumonia'], \n",
    "                yticklabels=['Normal', 'Pneumonia'])\n",
    "    plt.title(title)\n",
    "    plt.ylabel('True Label')\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.show()\n",
    "\n",
    "plot_confusion_matrix(true_labels, baseline_pred_binary, 'Baseline Model Confusion Matrix')\n",
    "plot_confusion_matrix(true_labels, augmented_pred_binary, 'Augmented Model Confusion Matrix')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15927e8c",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Visualize some correct and incorrect predictions\n",
    "def visualize_predictions(model, generator, num_images=5):\n",
    "    # Get a batch of data\n",
    "    x_batch, y_batch = next(generator)\n",
    "    \n",
    "    # Make predictions\n",
    "    preds = model.predict(x_batch, verbose=0)\n",
    "    pred_classes = (preds > 0.5).astype(int).flatten()\n",
    "    \n",
    "    # Get true classes\n",
    "    true_classes = y_batch.astype(int)\n",
    "    \n",
    "    # Find correct and incorrect predictions\n",
    "    correct_indices = np.where(pred_classes == true_classes)[0]\n",
    "    incorrect_indices = np.where(pred_classes != true_classes)[0]\n",
    "    \n",
    "    # Display correct predictions\n",
    "    print(\"Correct Predictions:\")\n",
    "    fig, axes = plt.subplots(1, min(num_images, len(correct_indices)), figsize=(15, 5))\n",
    "    if len(correct_indices) > 0:\n",
    "        for i, idx in enumerate(correct_indices[:num_images]):\n",
    "            axes[i].imshow(x_batch[idx])\n",
    "            axes[i].set_title(f'True: {true_classes[idx]}, Pred: {pred_classes[idx]}')\n",
    "            axes[i].axis('off')\n",
    "        plt.show()\n",
    "    \n",
    "    # Display incorrect predictions\n",
    "    print(\"Incorrect Predictions:\")\n",
    "    fig, axes = plt.subplots(1, min(num_images, len(incorrect_indices)), figsize=(15, 5))\n",
    "    if len(incorrect_indices) > 0:\n",
    "        for i, idx in enumerate(incorrect_indices[:num_images]):\n",
    "            axes[i].imshow(x_batch[idx])\n",
    "            axes[i].set_title(f'True: {true_classes[idx]}, Pred: {pred_classes[idx]}')\n",
    "            axes[i].axis('off')\n",
    "        plt.show()\n",
    "\n",
    "# Reset test generator to get a consistent batch\n",
    "test_generator.reset()\n",
    "visualize_predictions(baseline_model, test_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4e4d35e",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Create a comparison table\n",
    "comparison_data = {\n",
    "    'Model': ['Baseline', 'Strong Augmentation'],\n",
    "    'Test Accuracy': [baseline_results[1], augmented_results[1]],\n",
    "    'Test Loss': [baseline_results[0], augmented_results[0]],\n",
    "    'Precision': [baseline_results[2], augmented_results[2]],\n",
    "    'Recall': [baseline_results[3], augmented_results[3]]\n",
    "}\n",
    "\n",
    "comparison_df = pd.DataFrame(comparison_data)\n",
    "print(\"Model Comparison:\")\n",
    "print(comparison_df)\n",
    "\n",
    "# Plot comparison\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "metrics = ['Test Accuracy', 'Test Loss', 'Precision', 'Recall']\n",
    "colors = ['skyblue', 'lightcoral']\n",
    "\n",
    "for i, metric in enumerate(metrics):\n",
    "    row, col = i // 2, i % 2\n",
    "    axes[row, col].bar(comparison_df['Model'], comparison_df[metric], color=colors)\n",
    "    axes[row, col].set_title(metric)\n",
    "    axes[row, col].set_ylabel(metric)\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for j, v in enumerate(comparison_df[metric]):\n",
    "        axes[row, col].text(j, v + 0.01, f'{v:.3f}', ha='center', va='bottom')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Final summary\n",
    "print(\"\\nFINAL SUMMARY:\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Baseline Model achieved {baseline_results[1]*100:.2f}% accuracy on the test set.\")\n",
    "print(f\"Augmented Model achieved {augmented_results[1]*100:.2f}% accuracy on the test set.\")\n",
    "\n",
    "if augmented_results[1] > baseline_results[1]:\n",
    "    improvement = ((augmented_results[1] - baseline_results[1]) / baseline_results[1]) * 100\n",
    "    print(f\"Strong augmentation improved performance by {improvement:.2f}%.\")\n",
    "else:\n",
    "    print(\"Strong augmentation did not improve performance in this case.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a509deb5",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Save models\n",
    "baseline_model.save('pneumonia_baseline_cnn.h5')\n",
    "augmented_model.save('pneumonia_augmented_cnn.h5')\n",
    "print(\"Models saved successfully!\")\n",
    "\n",
    "# Save training history\n",
    "import pickle\n",
    "\n",
    "with open('baseline_history.pkl', 'wb') as f:\n",
    "    pickle.dump(baseline_history.history, f)\n",
    "    \n",
    "with open('augmented_history.pkl', 'wb') as f:\n",
    "    pickle.dump(augmented_history.history, f)\n",
    "    \n",
    "print(\"Training history saved successfully!\")\n",
    "\n",
    "# Save results to CSV\n",
    "comparison_df.to_csv('model_comparison.csv', index=False)\n",
    "print(\"Results saved to CSV!\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
